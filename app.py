import os
import json
import pickle
import joblib
import pandas as pd
from flask import Flask, jsonify, request
from peewee import (
    BooleanField, CharField, Model, IntegerField, FloatField,
    TextField, IntegrityError
)
from playhouse.shortcuts import model_to_dict
from playhouse.db_url import connect


########################################
# Begin database stuff

# The connect function checks if there is a DATABASE_URL env var.
# If it exists, it uses it to connect to a remote postgres db.
# Otherwise, it connects to a local sqlite db stored in predictions.db.
DB = connect(os.environ.get('DATABASE_URL') or 'sqlite:///predictions.db')

class Prediction(Model):
    observation_id = CharField(unique=True)
    observation = TextField()
    predict = BooleanField(default=True)
    proba = FloatField()
    #true_class = IntegerField(null=True)

    class Meta:
        database = DB


DB.create_tables([Prediction], safe=True)

# End database stuff
########################################

########################################
# Unpickle the previously-trained model


with open('columns.json') as fh:
    columns = json.load(fh)

pipeline = joblib.load('pipeline.pickle')

with open('dtypes.pickle', 'rb') as fh:
    dtypes = pickle.load(fh)


# End model un-pickling
########################################


########################################
# Begin webserver stuff

app = Flask(__name__)


from flask import jsonify, request
import pandas as pd

def attempt_predict(request, columns, dtypes, pipeline):
    """
    Produce prediction for request.
    
    Inputs:
        request: dictionary with the format described below
        {
            "observation_id": <id-as-a-string>,
            "data": {
                "age": <value>,
                "sex": <value>,
                "race": <value>,
                "workclass": <value>,
                "education": <value>,
                "marital-status": <value>,
                "capital-gain": <value>,
                "capital-loss": <value>,
                "hours-per-week": <value>,
            }
        }
     
    Returns: A dictionary with predictions or an error, the two potential values:
                if the request is OK and was properly parsed and predicted:
                {
                    "observation_id": <id-of-request>,
                    "prediction": <True|False>,
                    "probability": <probability generated by model>
                }
                otherwise:
                {
                    "observation_id": <id-of-request>,
                    "error": "some error message"
                }
    """
    # Feature constraints
    valid_sexes = {"male", "female"}
    valid_races = {'White', 'Black', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other'}
    min_age, max_age = 0, 120
    min_hours, max_hours = 0, 168  # 168 = 24h * 7d
    min_capital, max_capital = 0, float("inf")

    observation_id = request.get("observation_id", None)

    # Validate observation_id
    if not observation_id:
        return {
            "observation_id": None,
            "error": "Missing observation_id"
        }
    
    try:
        # Check columns
        request_columns = set(request["data"].keys())
        expected_columns = set(columns)
        missing_columns = expected_columns - request_columns
        extra_columns = request_columns - expected_columns

        if missing_columns or extra_columns:
            error_message = []
            if missing_columns:
                error_message.append(f"Missing columns: {' , '.join(missing_columns)}")
            if extra_columns:
                error_message.append(f"Extra columns: {' , '.join(extra_columns)}")
            
            return {
                "observation_id": observation_id,
                "error": " | ".join(error_message)
            }
        
        # Validate features values
        data = request["data"]
        errors = []

        if not (min_age <= data["age"] <= max_age):
            errors.append(f"Invalid age: {data['age']} (must be between {min_age} and {max_age})")

        if data["sex"].strip().lower() not in valid_sexes:
            errors.append(f"Invalid sex: {data['sex']} (must be 'male' or 'female')")
        
        if data["race"].strip().lower() not in map(str.lower, valid_races):
            errors.append(f"Invalid race: {data['race']} (must be one of {', '.join(valid_races)})")

        if not (min_capital <= data["capital-gain"] <= max_capital):
            errors.append(f"Invalid capital-gain: {data['capital-gain']} (must be non-negative)")

        if not (min_capital <= data["capital-loss"] <= max_capital):
            errors.append(f"Invalid capital-loss: {data['capital-loss']} (must be non-negative)")

        if not (min_hours <= data["hours-per-week"] <= max_hours):
            errors.append(f"Invalid hours-per-week: {data['hours-per-week']} (must be between {min_hours} and {max_hours})")

        if errors:
            return {
                "observation_id": observation_id,
                "error": " | ".join(errors)
            }
        
        # Convert input data into a DataFrame with the correct columns and types
        obs = pd.DataFrame([data], columns=columns)
        obs = obs.astype(dtypes)
        
        # Generate the prediction probabilities and the prediction itself
        probabilities = pipeline.predict_proba(obs)[0]
        prediction = pipeline.predict(obs)[0]
        
        # Format the response
        response = {
            "observation_id": observation_id,
            "prediction": bool(prediction),  # Convert prediction to a boolean (True/False)
            "probability": max(probabilities)  # Extract the highest probability (for the predicted class)
        }
        return response
    
    except Exception:
        # Handle errors by returning an error message
        return {
            "observation_id": observation_id,
            "error": "Missing data"
        }


@app.route('/predict', methods=['POST'])
def predict():
    """
    Endpoint to handle prediction requests.
    Receives a JSON payload with an 'id' and 'observation', processes the observation,
    and returns the predicted probability.
    """
    # Deserialize the JSON payload from the request
    obs_dict = request.get_json()

    result = attempt_predict(obs_dict, columns, dtypes, pipeline)
    print(f"\n\nFunction result:\n{result}\n\n")
    #print(f"\n\nPrediction:\n{type(result['prediction'])}\n\n")
    #print(f"\n\nProbability:\n{type(result['probability'])}\n\n")
    if "error" in result:
        return jsonify(result), 200
    
    else:
        # Guarda a previsÃ£o na base de dados
        p = Prediction.create(
            observation_id=result['observation_id'],
            prediction=1 if result['prediction'] == True else 0,
            proba=float(result['probability']),
            observation=json.dumps(obs_dict['data'])  # Guarda os dados como JSON
        )
        try:
            # Save the prediction to the database
            p.save()
        except IntegrityError:
            # Handle the case where the observation ID already exists in the database
            error_msg = f"Observation ID {result['observation_id']} already exists"
            result['error'] = error_msg
            print(error_msg)
            DB.rollback()  # Rollback the transaction to avoid partial updates
    
        return jsonify(result), 200

@app.route('/update', methods=['POST'])
def update():
    """
    Endpoint to update the true class of an existing observation.
    Receives a JSON payload with 'id' and 'true_class', updates the corresponding record in the database,
    and returns the updated observation.
    If the observation ID does not exist, returns an appropriate error message.
    """
    # Deserialize the JSON payload from the request
    obs = request.get_json()

    # Validate that the JSON contains the required fields: 'id' and 'true_class'
    if not obs or 'id' not in obs or 'true_class' not in obs:
        return jsonify({"error": "Invalid request: 'id' and 'true_class' are required"}), 400

    try:
        # Retrieve the prediction record from the database using the provided 'id'
        p = Prediction.get(Prediction.observation_id == obs['id'])

        # Update the 'true_class' field with the value from the request
        p.true_class = obs['true_class']

        # Save the updated record to the database
        p.save()

        # Return the updated observation as a JSON response
        return jsonify(model_to_dict(p))
    except Prediction.DoesNotExist:
        # Handle the case where the observation ID does not exist in the database
        error_msg = f"Observation ID {obs['id']} does not exist"
        return jsonify({'error': error_msg}), 404
    except Exception as e:
        # Handle any other unexpected errors
        return jsonify({"error": f"An error occurred: {str(e)}"}), 500


@app.route('/list-db-contents')
def list_db_contents():
    return jsonify([
        model_to_dict(obs) for obs in Prediction.select()
    ])


# End webserver stuff
########################################

if __name__ == "__main__":
    app.run(host='0.0.0.0', debug=True, port=5000)